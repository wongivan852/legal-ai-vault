# ==============================================================================
# Vault AI Platform - Production Configuration for Linux GPU Server
# ==============================================================================
#
# Server Configuration: Linux with 4 NVIDIA GPUs
# Target Model: llama3.3:70b (GPU-accelerated)
#
# IMPORTANT: Copy this file to .env and fill in the secure values
# Generate secrets with: openssl rand -hex 32
# ==============================================================================

# ------------------------------------------------------------------------------
# DATABASE CONFIGURATION
# ------------------------------------------------------------------------------
DB_USER=legal_vault_user
DB_PASSWORD=CHANGE_ME_GENERATE_SECURE_PASSWORD_HERE

# PostgreSQL Performance Tuning (for server with adequate RAM)
POSTGRES_SHARED_BUFFERS=2GB
POSTGRES_EFFECTIVE_CACHE_SIZE=6GB
POSTGRES_MAX_CONNECTIONS=200

# PostgreSQL Port (default: 5432, change if needed)
POSTGRES_PORT=5432

# ------------------------------------------------------------------------------
# SECURITY KEYS
# ------------------------------------------------------------------------------
# Generate with: openssl rand -hex 32
JWT_SECRET=CHANGE_ME_GENERATE_WITH_OPENSSL_RAND_HEX_32
ENCRYPTION_KEY=CHANGE_ME_GENERATE_WITH_OPENSSL_RAND_HEX_32

# ------------------------------------------------------------------------------
# OLLAMA CONFIGURATION (GPU-Optimized)
# ------------------------------------------------------------------------------
# Ollama API endpoint
OLLAMA_URL=http://ollama:11434

# Primary LLM Model - llama3.3:70b recommended for 4-GPU server
# Alternatives: llama3.1:70b, llama3.1:8b (for testing)
OLLAMA_MODEL=llama3.3:70b

# Embedding Model - nomic-embed-text recommended
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

# Ollama Models Storage Path
# Option 1: Use Docker named volume (recommended for production)
# OLLAMA_MODELS_PATH=ollama_models
#
# Option 2: Mount existing Ollama models directory from host (Linux)
# OLLAMA_MODELS_PATH=/home/youruser/.ollama
#
# Option 3: Mount from dedicated storage mount
# OLLAMA_MODELS_PATH=/mnt/data/ollama_models
OLLAMA_MODELS_PATH=ollama_models

# GPU Configuration
# Use all 4 GPUs (or specify count: 1, 2, 3, 4)
GPU_COUNT=all

# ------------------------------------------------------------------------------
# VECTOR DATABASE (QDRANT)
# ------------------------------------------------------------------------------
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Qdrant External Ports (for direct access, secure with firewall)
QDRANT_HTTP_PORT=6333
QDRANT_GRPC_PORT=6334

# ------------------------------------------------------------------------------
# APPLICATION SETTINGS
# ------------------------------------------------------------------------------
# Debug Mode (ALWAYS false in production)
DEBUG=false

# Logging Level (INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# API Server Port
API_PORT=8000

# API Workers (set to number of CPU cores, typically 4-8 for GPU server)
API_WORKERS=4

# Max Concurrent Requests (adjust based on GPU memory and model size)
# For llama3.3:70b with 4 GPUs: 50-100 concurrent requests
MAX_CONCURRENT_REQUESTS=100

# ------------------------------------------------------------------------------
# CORS CONFIGURATION
# ------------------------------------------------------------------------------
# Allowed Origins (comma-separated list)
# For production: specify your domain(s)
# Example: https://yourdomain.com,https://www.yourdomain.com
# For development: http://localhost:8000
# For internal network: http://192.168.1.100:8000
ALLOWED_ORIGINS=http://localhost:8000

# ------------------------------------------------------------------------------
# NETWORK CONFIGURATION (Production with Nginx)
# ------------------------------------------------------------------------------
# Server domain name (for SSL/TLS)
# SERVER_NAME=yourdomain.com

# SSL Certificate paths (if using nginx with SSL)
# SSL_CERT_PATH=/etc/nginx/ssl/cert.pem
# SSL_KEY_PATH=/etc/nginx/ssl/key.pem

# ------------------------------------------------------------------------------
# PERFORMANCE & MONITORING
# ------------------------------------------------------------------------------
# Enable performance monitoring (optional)
# ENABLE_METRICS=true
# METRICS_PORT=9090

# Request timeout (seconds) - increase for large LLM responses
# REQUEST_TIMEOUT=600

# Max tokens for LLM generation (adjust based on use case)
# MAX_TOKENS=4096

# ------------------------------------------------------------------------------
# BACKUP & DATA RETENTION
# ------------------------------------------------------------------------------
# Backup directory (for database dumps)
# BACKUP_DIR=/mnt/backup/legal-ai-vault

# Log retention (days)
# LOG_RETENTION_DAYS=30

# ------------------------------------------------------------------------------
# GPU OPTIMIZATION SETTINGS
# ------------------------------------------------------------------------------
# GPU Memory Fraction (0.0 to 1.0) - how much GPU memory to use
# OLLAMA_GPU_MEMORY_FRACTION=0.9

# Number of GPU layers to offload (auto-detected by Ollama)
# Higher = more GPU utilization, faster inference
# OLLAMA_NUM_GPU=all

# Batch size for parallel processing
# OLLAMA_BATCH_SIZE=512

# ------------------------------------------------------------------------------
# SYSTEM RESOURCE LIMITS
# ------------------------------------------------------------------------------
# Docker resource limits (uncomment and adjust as needed)
# API_MEMORY_LIMIT=8g
# API_CPU_LIMIT=4.0
# POSTGRES_MEMORY_LIMIT=4g
# QDRANT_MEMORY_LIMIT=4g

# ------------------------------------------------------------------------------
# NOTES FOR 4-GPU SERVER DEPLOYMENT
# ------------------------------------------------------------------------------
#
# Recommended Hardware:
# - CPU: 16+ cores
# - RAM: 64GB+ (128GB recommended for llama3.3:70b)
# - GPU: 4x NVIDIA GPUs with 24GB+ VRAM each (A100, A6000, RTX 4090)
# - Storage: 500GB+ SSD for models and data
# - Network: 10Gbps for high-throughput scenarios
#
# Pre-deployment Checklist:
# 1. Install NVIDIA drivers (version 525+)
# 2. Install NVIDIA Container Toolkit
# 3. Pull Ollama models: ollama pull llama3.3:70b
# 4. Configure firewall (UFW): allow ports 80, 443, 8000
# 5. Set up SSL certificates (Let's Encrypt)
# 6. Configure nginx reverse proxy
# 7. Set up automated backups
# 8. Enable monitoring (Prometheus + Grafana)
#
# Performance Expectations (4x GPU, llama3.3:70b):
# - First query: 10-30 seconds (model loading)
# - Subsequent queries: 5-15 seconds (depending on complexity)
# - Concurrent users: 50-100 (with proper GPU memory management)
# - Throughput: 10-20 tokens/second per GPU
#
# ==============================================================================
