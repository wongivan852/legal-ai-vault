version: '3.8'

services:
  # Ollama service - GPU-accelerated LLM inference
  # Optimized for Linux server with 4 NVIDIA GPUs
  ollama:
    image: ollama/ollama:latest
    container_name: legal-ai-ollama
    volumes:
      # Use environment variable for Ollama models path (Linux compatible)
      # Default: /root/.ollama for Docker volume, or set OLLAMA_MODELS_PATH in .env
      - ${OLLAMA_MODELS_PATH:-ollama_models}:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - legal-ai-network
    # GPU support - ENABLED for 4 NVIDIA GPUs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-all}  # Use all 4 GPUs or specify count in .env
              capabilities: [gpu]
    environment:
      # GPU optimization settings
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    # Health check: Use nc (netcat) instead of curl
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: legal-ai-postgres
    environment:
      POSTGRES_DB: legal_ai_vault
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      # Performance tuning for production
      POSTGRES_SHARED_BUFFERS: ${POSTGRES_SHARED_BUFFERS:-2GB}
      POSTGRES_EFFECTIVE_CACHE_SIZE: ${POSTGRES_EFFECTIVE_CACHE_SIZE:-6GB}
      POSTGRES_MAX_CONNECTIONS: ${POSTGRES_MAX_CONNECTIONS:-200}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      # Bind to all interfaces in production (use firewall for security)
      - "${POSTGRES_PORT:-5432}:5432"
    restart: unless-stopped
    networks:
      - legal-ai-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-legal_vault_user}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:v1.8.0
    container_name: legal-ai-qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      # Expose for external access (secure with firewall/nginx)
      - "${QDRANT_HTTP_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    restart: unless-stopped
    networks:
      - legal-ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 10s
      timeout: 5s
      retries: 5

  # FastAPI Backend
  api:
    build: ./api
    container_name: legal-ai-api
    environment:
      # Database (NO DEFAULTS - must be set in .env)
      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/legal_ai_vault

      # Vector Database
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333

      # Ollama connection - optimized for GPU server with llama3.3:70b
      OLLAMA_URL: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.3:70b}
      OLLAMA_EMBEDDING_MODEL: ${OLLAMA_EMBEDDING_MODEL:-nomic-embed-text:latest}

      # Security (NO DEFAULTS - must be set in .env)
      JWT_SECRET: ${JWT_SECRET}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY}

      # CORS Configuration
      ALLOWED_ORIGINS: ${ALLOWED_ORIGINS:-http://localhost:8000}

      # Application
      DEBUG: ${DEBUG:-false}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # Performance settings for GPU server
      WORKERS: ${API_WORKERS:-4}
      MAX_CONCURRENT_REQUESTS: ${MAX_CONCURRENT_REQUESTS:-100}
    volumes:
      - ./api:/app
      - ./frontend:/app/frontend
      - ./data:/app/data:ro
      - api_documents:/app/documents
      - api_logs:/app/logs
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      - postgres
      - qdrant
      - ollama
    restart: unless-stopped
    networks:
      - legal-ai-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Nginx Reverse Proxy (optional, for production)
  nginx:
    image: nginx:alpine
    container_name: legal-ai-nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - legal-ai-network
    profiles:
      - production

volumes:
  postgres_data:
    driver: local
  qdrant_data:
    driver: local
  api_documents:
    driver: local
  api_logs:
    driver: local
  ollama_models:
    driver: local
    # For production, you can mount external storage here
    # driver_opts:
    #   type: none
    #   o: bind
    #   device: /mnt/data/ollama_models

networks:
  legal-ai-network:
    driver: bridge
